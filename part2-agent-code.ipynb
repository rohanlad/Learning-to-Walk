{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA38jtUgtZsG"
      },
      "outputs": [],
      "source": [
        "# CITATIONS: https://github.com/soumik12345/Twin-Delayed-DDPG\n",
        "\n",
        "%%capture\n",
        "!apt update\n",
        "!pip install 'gym[box2d]'\n",
        "!python --version\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 25 # videos can take a very long time to render so only do it every N episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reinforcement learning agent**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "\n",
        "    def __init__(\n",
        "            self, state_dim, action_dim, max_action, device,\n",
        "            discount=0.99, rho=0.995, policy_noise=0.2, noise_clip=0.5, update_policy_delay=2):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.0003)\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.0003)\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.rho = rho\n",
        "        self.device = device\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.update_policy_delay = update_policy_delay\n",
        "        self.total_it = 0\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        s = torch.FloatTensor(s.reshape(1, -1)).to(self.device)\n",
        "        return self.actor(s).cpu().data.numpy().flatten()\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_update(local_model, target_model, rho):\n",
        "        for param, target_param in zip(local_model.parameters(), target_model.parameters()):\n",
        "            target_param.data.copy_((1-rho) * param.data + rho * target_param.data)\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=100):\n",
        "        self.total_it += 1\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "        with torch.no_grad():\n",
        "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "            target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "            target_q = torch.min(target_q1, target_q2)\n",
        "            target_q = reward + not_done * self.discount * target_q\n",
        "\n",
        "        current_q1, current_q2 = self.critic(state, action)\n",
        "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        if self.total_it % self.update_policy_delay == 0:\n",
        "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "            Agent.soft_update(self.critic, self.critic_target, self.rho)\n",
        "            Agent.soft_update(self.actor, self.actor_target, self.rho)\n",
        "        \n",
        "class Actor(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(state_dim, 256)\n",
        "        self.l2 = torch.nn.Linear(256, 256)\n",
        "        self.l3 = torch.nn.Linear(256, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        return self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = torch.nn.Linear(256, 256)\n",
        "        self.l3 = torch.nn.Linear(256, 1)\n",
        "        self.l4 = torch.nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l5 = torch.nn.Linear(256, 256)\n",
        "        self.l6 = torch.nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = F.relu(self.l1(sa))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        q1 = self.l3(q1)\n",
        "        q2 = F.relu(self.l4(sa))\n",
        "        q2 = F.relu(self.l5(q2))\n",
        "        q2 = self.l6(q2)\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = F.relu(self.l1(sa))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        q1 = self.l3(q1)\n",
        "        return q1\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, device, max_size=int(1500000)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "        self.state = np.zeros((max_size, obs_dim))\n",
        "        self.action = np.zeros((max_size, act_dim))\n",
        "        self.next_state = np.zeros((max_size, obs_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.not_done = np.zeros((max_size, 1))\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "        return (\n",
        "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "# env = gym.make(\"Pendulum-v0\") # useful continuous environment for quick experiments\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUw4h980jfnu",
        "outputId": "bf4da728-bf06-4bfe-ea3f-6d7cf15f2e77"
      },
      "outputs": [],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# in the submission please use seed 42 for verification\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(\n",
        "    state_dim=obs_dim, action_dim=act_dim,\n",
        "    max_action=float(env.action_space.high[0]), device=device,\n",
        "    discount=0.99, rho=0.995\n",
        ")\n",
        "max_episodes = 1250\n",
        "max_timesteps = 2000\n",
        "memory = ReplayBuffer(obs_dim, act_dim, device=device)\n",
        "\n",
        "# training procedure\n",
        "ts = 0\n",
        "for episode in range(1, max_episodes+1):\n",
        "    state = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        ts += 1\n",
        "\n",
        "        # select the agent action\n",
        "        if ts < 18000:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = (\n",
        "            agent.sample_action(np.array(state)) + np.random.normal(\n",
        "                0, float(env.action_space.high[0]) * 0.1,\n",
        "                size=act_dim\n",
        "            )\n",
        "        ).clip(\n",
        "            -float(env.action_space.high[0]),\n",
        "            float(env.action_space.high[0])\n",
        "        )\n",
        "\n",
        "        # take action in environment and get r and s'\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        memory.add(\n",
        "        state, action, next_state, reward,\n",
        "        float(done) if t < 2000 else 0)\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "\n",
        "        if ts >= 18000:\n",
        "            agent.train(memory, 256)\n",
        "\n",
        "        # stop iterating when the episode finished\n",
        "        if done or t==(max_timesteps-1):\n",
        "            break\n",
        "    \n",
        "    # append the episode reward to the reward list\n",
        "    reward_list.append(ep_reward)\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "    log_f.flush()\n",
        "    ep_reward = 0\n",
        "\n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RL-Assignment",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
